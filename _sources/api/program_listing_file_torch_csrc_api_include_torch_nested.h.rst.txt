:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file_torch_csrc_api_include_torch_nested.h:

Program Listing for File nested.h
=================================

|exhale_lsh| :ref:`Return to documentation for file <file_torch_csrc_api_include_torch_nested.h>` (``torch/csrc/api/include/torch/nested.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   #pragma once
   
   #include <ATen/ATen.h>
   #include <torch/torch.h>
   
   namespace torch {
   namespace nested {
   
   inline Tensor nested_tensor(
       TensorList list,
       c10::optional<ScalarType> dtype = c10::nullopt,
       c10::optional<Device> device = c10::nullopt,
       c10::optional<bool> requires_grad = false,
       c10::optional<bool> pin_memory = false) {
     std::vector<Tensor> new_list;
     for (const auto i : c10::irange(list.size())) {
       new_list.push_back(list[i].clone().detach());
     }
     auto out = torch::_nested_tensor_from_tensor_list(
         new_list, dtype, c10::nullopt, device, pin_memory);
     if (requires_grad.has_value() && requires_grad.value()) {
       out.requires_grad_(true);
     }
     return out;
   }
   
   inline Tensor as_nested_tensor(
       TensorList list,
       c10::optional<ScalarType> dtype = c10::nullopt,
       c10::optional<Device> device = c10::nullopt) {
     return at::_nested_tensor_from_tensor_list(
         list, dtype, c10::nullopt, device, c10::nullopt);
   }
   
   inline Tensor to_padded_tensor(
       const Tensor& self,
       double padding,
       OptionalIntArrayRef output_size = c10::nullopt) {
     return torch::nested_to_padded_tensor(self, padding, output_size);
   }
   
   } // namespace nested
   } // namespace torch
